<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> – Features</title>
    <link>/docs/concepts/</link>
    <description>Recent content in Features on </description>
    <generator>Hugo -- gohugo.io</generator>
    
	  <atom:link href="/docs/concepts/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Docs: Ondat Cluster Topologies</title>
      <link>/docs/concepts/cluster-topologies/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/concepts/cluster-topologies/</guid>
      <description>
        
        
        &lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Ondat makes it possible for cluster administrators to design and implement different cluster topologies, depending on types of workloads, use cases, priorities and needs. The topology approaches recommended below are idealised representations of possible Ondat clusters and can be mixed, modified and changed at execution time.&lt;/p&gt;
&lt;p&gt;Ondat performs file Input/Output (I/O) operations over the network, which is how the platform ensures that data is always available throughout your cluster. This also affords cluster administrators certain possibilities of organising their clusters in ways explained below.&lt;/p&gt;
&lt;h3 id=&#34;hyper-converged-cluster-topology&#34;&gt;Hyper-converged Cluster Topology&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;/images/docs/concepts/hyperconverged.png&#34; alt=&#34;Hyper-converged Cluster Topology&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;a href=&#34;https://en.wikipedia.org/wiki/Hyper-converged_infrastructure&#34;&gt;&lt;em&gt;hyper-converged&lt;/em&gt;&lt;/a&gt; cluster topology model leverages the available block storage attached to all the worker nodes in a Kubernetes cluster, creating a single storage pool that stores and present data for stateful workloads deployed and running.
&lt;ul&gt;
&lt;li&gt;This cluster topology gives the best flexibility to Ondat and Kubernetes schedulers, and provides maximum choice for optimal pod placement when pods are being assigned to nodes in a cluster.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;No matter how or where workloads are deployed on worker nodes, Ondat will ensure that the data from workloads is stored, persistent and always accessible.&lt;/li&gt;
&lt;li&gt;New Ondat deployments will place workloads locally where possible using this hyper-converged cluster topology out of the box.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;centralised-cluster-topology&#34;&gt;Centralised Cluster Topology&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;/images/docs/concepts/centralised.png&#34; alt=&#34;Centralised Cluster Topology&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;em&gt;centralised&lt;/em&gt; cluster topology model leverages the available block storage attached to only a &lt;em&gt;subset&lt;/em&gt; of worker nodes (creating a dedicated, storage-optimised &lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/concepts/node-pools&#34;&gt;node pool&lt;/a&gt;) in a Kubernetes cluster, whilst the rest of the worker nodes are dedicated to running general and compute-intensive workloads,
&lt;ul&gt;
&lt;li&gt;Deployed workloads in centralised cluster that require data persistency will access a dedicated storage pool that is located on the declared subset of worker nodes.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;This cluster topology can be beneficial if, for example, cluster administers want to take advantage and effectively utilise high performance-optimised hardware components of a particular set of worker nodes for different types of workloads being deployed.&lt;/li&gt;
&lt;li&gt;The cluster topology can also aid in avoiding downtime issues that can arise from unaccounted resource/capacity planning and allocation for workloads, since storage-optimised nodes and compute-optimised workloads are compartmentalised.&lt;/li&gt;
&lt;li&gt;In addition, another suitable use case for this topology is for elastic worker node fleets with burst-able workloads. A fleet can be quickly expanded with new worker nodes for compute-intensive workloads on demand, whilst maintaining a centralised data storage pool that is not impacted by rapid auto cluster scaling.&lt;/li&gt;
&lt;li&gt;To configure this cluster topology for a new Ondat deployment, cluster administrators would need to apply an Ondat node label called &lt;code&gt;storageos.com/computeonly&lt;/code&gt; to nodes, which would inform Ondat that it &lt;em&gt;should not&lt;/em&gt; use the nodes to join a storage pool.
&lt;ul&gt;
&lt;li&gt;Review the &lt;a href=&#34;/docs/operations/compute-only/&#34;&gt;Centralised Cluster Topology&lt;/a&gt; operations page for more information on how to use this topology model for your clusters.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Ondat Components</title>
      <link>/docs/concepts/components/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/concepts/components/</guid>
      <description>
        
        
        &lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Ondat is a software-defined storage platform for running stateful applications in Kubernetes.&lt;/p&gt;
&lt;p&gt;Fundamentally, Ondat uses the storage attached to the nodes in the Ondat cluster to create and present virtual volumes into containers.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Space on the host is consumed from the mount point &lt;code&gt;/var/lib/storageos/data&lt;/code&gt; - so it is recommended that &lt;a href=&#34;https://en.wikipedia.org/wiki/Disk_storage&#34;&gt;disk devices&lt;/a&gt; are used exclusively for Ondat, as described in the &lt;a href=&#34;/docs/operations/managing-host-storage&#34;&gt;Managing Host Storage&lt;/a&gt; operations page.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ondat is agnostic to the underlying storage and runs equally well on bare metal, in virtual machines or on cloud providers.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/docs/concepts/ondat-deployment.png&#34; alt=&#34;Ondat cluster Components Diagram&#34;&gt;&lt;/p&gt;
&lt;p&gt;Read about &lt;a href=&#34;https://www.ondat.io/platform/platform-overview&#34;&gt;the cloud native storage principles behind Ondat&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-native-ondat-components&#34;&gt;Kubernetes-native Ondat Components&lt;/h2&gt;
&lt;p&gt;Ondat is architected as a series of containers that fulfil separate, discrete functions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Below is a list of core Ondat components with a description for each components responsibilities &amp;amp; tasks:&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ondat-cluster-operator&#34;&gt;Ondat Cluster Operator&lt;/h3&gt;
&lt;p&gt;The &lt;a href=&#34;https://github.com/storageos/operator&#34;&gt;&lt;strong&gt;Ondat Cluster Operator&lt;/strong&gt;&lt;/a&gt; is responsible for the creation and maintenance of the Ondat cluster.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This operator is primarily responsible for ensuring that all the relevant applications are running in your cluster.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ondat-api-manager&#34;&gt;Ondat API Manager&lt;/h3&gt;
&lt;p&gt;The &lt;a href=&#34;https://github.com/storageos/api-manager&#34;&gt;&lt;strong&gt;Ondat API Manager&lt;/strong&gt;&lt;/a&gt; acts as a middle-man between various APIs. It has all the capabilities of a &lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/operator/&#34;&gt;Kubernetes operator&lt;/a&gt; and is also able to communicate with the Ondat control plane API.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This application handles typical operator tasks like labelling or removing nodes from Ondat when removed from the Kubernetes. It is continually monitoring the state of the cluster and moving it towards the desired state when necessary.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ondat-data-plane&#34;&gt;Ondat Data Plane&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;Ondat Data Plane&lt;/strong&gt; is responsible for all &lt;a href=&#34;https://en.wikipedia.org/wiki/Input/output&#34;&gt;I/O operations&lt;/a&gt; path related tasks;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Reading_%28computer%29&#34;&gt;Reading&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Read%E2%80%93write_memory&#34;&gt;Writing&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Data_compression&#34;&gt;Compression&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Cache_%28computing%29&#34;&gt;Caching&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ondat-control-plane&#34;&gt;Ondat Control Plane&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;Ondat Control Plane&lt;/strong&gt; is responsible for monitoring and maintaining the state of volumes and nodes in the cluster.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Control Plane and the Data Plane run together in a single container, managed by a daemonset.&lt;/li&gt;
&lt;li&gt;The Control Plane works with a dedicated &lt;a href=&#34;https://etcd.io/&#34;&gt;&lt;code&gt;etcd&lt;/code&gt;&lt;/a&gt; instance to maintain state consensus in your cluster.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ondat-scheduler&#34;&gt;Ondat Scheduler&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;Ondat Scheduler&lt;/strong&gt; is responsible for scheduling applications on the same node as an application&amp;rsquo;s
volumes.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ondat uses a custom &lt;a href=&#34;https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/&#34;&gt;Kubernetes scheduler&lt;/a&gt; to handle pod placement, ensuring that volumes are deployed on the same nodes as the relevant workloads as often as possible.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ondat-csi-helper&#34;&gt;Ondat CSI Helper&lt;/h3&gt;
&lt;p&gt;The &lt;a href=&#34;https://github.com/storageos/external-provisioner&#34;&gt;&lt;strong&gt;CSI Helper&lt;/strong&gt;&lt;/a&gt; is responsible for registering Ondat with Kubernetes as a CSI driver.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It is necessary because the internal persistent volume controller running in Kubernetes controller-manager does not have any direct interfaces to CSI drivers.&lt;/li&gt;
&lt;li&gt;It monitors PVC objects created by users and creates/deletes volumes for them.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ondat-node-guard&#34;&gt;Ondat Node Guard&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;Ondat Node Guard&lt;/strong&gt; is a key component of the &lt;a href=&#34;/docs/concepts/rolling-upgrades/&#34;&gt;Ondat Rolling Upgrade Protection for Orchestrators&lt;/a&gt; feature. It blocks certain nodes from being upgraded or drained thus avoiding data loss in the cluster.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Node Guard will detect if a volume is reconciling (for example, one that does not have enough synced replicas), at which point a node manager pod on the same node as the reconciling volume&amp;rsquo;s master and replicas become unready.&lt;/li&gt;
&lt;li&gt;Ondat uses a &lt;a href=&#34;https://kubernetes.io/docs/tasks/run-application/configure-pdb/&#34;&gt;PodDisruptionBudget (PDB)&lt;/a&gt; to stop more than &lt;code&gt;1&lt;/code&gt; node manager pod being unavailable at any point in time. This prevents the rolling upgrade from continuing until the PDB is satisfied and all volumes have fully reconciled.&lt;/li&gt;
&lt;li&gt;If the PDB is set to &lt;code&gt;1&lt;/code&gt; and a Control Plane volume on a node is not ready for a long period of time, this will stop the upgrade process. The &lt;code&gt;api-managercomponent&lt;/code&gt; will be able to dynamically set the PDB value if it can determine the health of the volume.&lt;/li&gt;
&lt;li&gt;If the &lt;code&gt;api-managercomponent&lt;/code&gt; knows that a volume will not be ready, it can increase the PDB &lt;code&gt;maxUnavailable&lt;/code&gt; value, allowing the upgrade to continue. The Node Guard container will log when it is available to upgrade, it will also log the reason if upgrade is not possible.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;⚠️ The Node Guard container only monitors volumes that host a deployment on its node (for example, it doesn’t care if a volume is unhealthy if the node it&amp;rsquo;s running on hosts none of the volumes primary and replicas)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;⚠️ There is some latency between a volume becoming unhealthy and the Node Guard noticing, due to the polling nature of both the &lt;code&gt;api-managercomponent&lt;/code&gt; volume sync Kubernetes readiness endpoints)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;ondat-node-manager&#34;&gt;Ondat Node Manager&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;Ondat Node Manager&lt;/strong&gt; is an out-of-band pod used for node management. It runs on all nodes that run the &lt;code&gt;StorageOS&lt;/code&gt; node container and is a separate pod so that it can be restarted independently of the node container.&lt;/p&gt;
&lt;h2 id=&#34;putting-it-all-together&#34;&gt;Putting It All Together&lt;/h2&gt;
&lt;p&gt;Ondat is deployed by the &lt;strong&gt;Ondat Cluster Operator&lt;/strong&gt;. In Kubernetes, the Ondat &lt;strong&gt;Control Plane&lt;/strong&gt; and &lt;strong&gt;Data Plane&lt;/strong&gt; are deployed in a single pod managed by a &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/&#34;&gt;daemonset&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This daemonset runs on every node in the cluster that will consume or present storage.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;strong&gt;Ondat Scheduler&lt;/strong&gt;, &lt;strong&gt;CSI Helper&lt;/strong&gt;, &lt;strong&gt;Cluster Operator&lt;/strong&gt; and &lt;strong&gt;API Manager&lt;/strong&gt; run as separate &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/pods/&#34;&gt;pods&lt;/a&gt; and are controlled as &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/deployment/&#34;&gt;deployments&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Ondat is designed to feel familiar to Kubernetes users. Storage is managed through standard &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/storage-classes/&#34;&gt;StorageClasses&lt;/a&gt; , &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/persistent-volumes/&#34;&gt;PersistentVolumeClaims&lt;/a&gt;, and &lt;a href=&#34;/docs/concepts/labels&#34;&gt;Ondat features&lt;/a&gt; are controlled by &lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/&#34;&gt;Kubernetes labels and selectors&lt;/a&gt;, prefixed with &lt;code&gt;storageos.com/&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;By default, volumes are cached to improve read performance and compressed to reduce network traffic.&lt;/li&gt;
&lt;li&gt;Any pod may mount an Ondat virtual volume from any node that is also running Ondat, regardless of whether the pod and volume are collocated on the same node. Therefore, applications may be started or restarted on any node and access volumes transparently.&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Ondat Compression</title>
      <link>/docs/concepts/compression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/concepts/compression/</guid>
      <description>
        
        
        &lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;💡 This feature is disabled by default in release &lt;code&gt;v2.2.0&lt;/code&gt; or greater.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;data-compression&#34;&gt;Data Compression&lt;/h3&gt;
&lt;p&gt;Ondat compression is handled on a per volume basis and is disabled in &lt;code&gt;v2.2.0&lt;/code&gt;, as performance is generally increased when compression is disabled due to &lt;a href=&#34;https://en.wikipedia.org/wiki/Data_structure_alignment&#34;&gt;block alignment&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This means that there is a trade off between volume performance and the space the volume occupies on the backend device.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ondat utilises the &lt;a href=&#34;https://en.wikipedia.org/wiki/LZ4_%28compression_algorithm%29&#34;&gt;LZ4 (compression algorithm)&lt;/a&gt; when writing to the backend store and when compressing Ondat &lt;a href=&#34;/docs/concepts/replication&#34;&gt;replication traffic&lt;/a&gt; before it is sent across the network.&lt;/p&gt;
&lt;p&gt;Ondat detects whether a block can be compressed or not by creating a heuristic that predicts the size of a compressed block.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the heuristic indicates that the compressed block is likely to be larger than the original block then the uncompressed block is stored.&lt;/li&gt;
&lt;li&gt;Block size increases post-compression if the compression dictionary is added to a block that cannot be compressed. By verifying whether blocks can be compressed, disk efficiency is increased and CPU resources are not wasted on attempts to compress incompressible blocks.&lt;/li&gt;
&lt;li&gt;Ondat&amp;rsquo;s patented on-disk format is used to tell whether individual blocks are compressed without overhead. As such volume compression can be dynamically enabled/disabled even while a volume is in use.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;how-to-enable-ondat-compression&#34;&gt;How To Enable Ondat Compression?&lt;/h3&gt;
&lt;p&gt;Compression can be enabled by setting the &lt;a href=&#34;/docs/concepts/labels&#34;&gt;Ondat Feature Label&lt;/a&gt; &amp;raquo; &lt;code&gt;storageos.com/nocompress=false&lt;/code&gt; on a volume at volume creation time.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For more information on how to enable compression, review the &lt;a href=&#34;/docs/operations/compression&#34;&gt;Data Compression&lt;/a&gt; operations page.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ondat-compression--data-encryption&#34;&gt;Ondat Compression &amp;amp; Data Encryption&lt;/h3&gt;
&lt;p&gt;When Ondat compression and &lt;a href=&#34;/docs/concepts/encryption&#34;&gt;data encryption&lt;/a&gt; are both enabled for a volume, blocks are &lt;strong&gt;compressed first&lt;/strong&gt; and then encrypted.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Ondat Data Encryption</title>
      <link>/docs/concepts/encryption/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/concepts/encryption/</guid>
      <description>
        
        
        &lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;💡 This feature is available in release &lt;code&gt;v2.4.0&lt;/code&gt; or greater.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;💡 End users can also leverage &lt;a href=&#34;https://www.ondat.io/trousseau&#34;&gt;Trousseau&lt;/a&gt; with Ondat&amp;rsquo;s volume encryption feature. Trousseau is an open source KMS plugin project that based on Kubernetes KMS provider design. The project allows users to store and access your secrets the Kubernetes native way with any external KMS. Trousseau&amp;rsquo;s repository can be located on &lt;a href=&#34;https://github.com/ondat/trousseau&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;data-encryption&#34;&gt;Data Encryption&lt;/h3&gt;
&lt;p&gt;Ondat supports &lt;a href=&#34;https://en.wikipedia.org/wiki/Data_at_rest&#34;&gt;data encryption-at-rest&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/Data_in_transit&#34;&gt;data encryption-in-transit&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data encryption-in-transit is data as it is travelling between nodes. It is encrypted by default with &lt;a href=&#34;https://en.wikipedia.org/wiki/Mutual_authentication&#34;&gt;Mutual Authentication (mTLS)&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Data encryption-at-rest is the data stored in your volumes as &lt;a href=&#34;/docs/concepts/volumes&#34;&gt;blob files&lt;/a&gt;. Encryption of these blob files is optional and can be enabled by adding a label to your volume definitions &lt;strong&gt;before they are provisioned&lt;/strong&gt; .&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For more information on how to enable data encryption for Ondat volumes, review the &lt;a href=&#34;/docs/operations/encryption/&#34;&gt;Ondat Data Encryption&lt;/a&gt; operations page.&lt;/p&gt;
&lt;h3 id=&#34;how-are-ondat-volumes-encrypted&#34;&gt;How Are Ondat Volumes Encrypted?&lt;/h3&gt;
&lt;p&gt;Volumes are encrypted using &lt;code&gt;AES-256&lt;/code&gt; in the &lt;code&gt;XTS-AES&lt;/code&gt; mode with &lt;code&gt;512-bit&lt;/code&gt; keys, as
specified by &lt;a href=&#34;https://standards.ieee.org/ieee/1619/4205/&#34;&gt;&lt;code&gt;IEEE Standard 1619-2007&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There is a non-zero performance impact of using encrypted volumes. A &lt;code&gt;10-25%&lt;/code&gt; cost in read/write throughput can be expected from &lt;code&gt;XTS-AES&lt;/code&gt;, dependent on workload.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Thin_provisioning&#34;&gt;Thin provisioning&lt;/a&gt; still applies to Ondat encrypted volumes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;how-are-ondat-encryption-keys-generated&#34;&gt;How Are Ondat Encryption Keys Generated?&lt;/h3&gt;
&lt;p&gt;On PVC creation, if data encryption-at-rest is enabled, Ondat will automatically generate &lt;strong&gt;up to two keys&lt;/strong&gt; as &lt;a href=&#34;https://kubernetes.io/docs/concepts/configuration/secret/&#34;&gt;Kubernetes secrets&lt;/a&gt;. Both keys are stored in the &lt;strong&gt;same namespace&lt;/strong&gt; as the PVC.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Firstly, if it doesn&amp;rsquo;t already exist, a &lt;strong&gt;namespace key&lt;/strong&gt; is generated. It is always named &lt;code&gt;storageos-namespace-key&lt;/code&gt; and &lt;strong&gt;only one exists per namespace&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Secondly a &lt;strong&gt;volume key&lt;/strong&gt; is created for each encrypted volume. It has a name in the format &lt;code&gt;storageos-volume-key-&amp;lt;random-id&amp;gt;&lt;/code&gt;, with no connection to the name of the volume.
&lt;ol&gt;
&lt;li&gt;The volume it is associated with can be determined by looking at the &lt;code&gt;storageos.com/pvc&lt;/code&gt; label on the secret.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;storageos.com/encryption-secret-name&lt;/code&gt; and &lt;code&gt;storageos.com/encryption-secret-namespace&lt;/code&gt; annotations are added to the PVC by an admission controller to map the PVC back to its secret.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;The encryption key is passed to Ondat as part of the CSI volume creation request and is used to encrypt the volume.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;how-are-encryption-keys-used&#34;&gt;How Are Encryption Keys Used?&lt;/h3&gt;
&lt;p&gt;The volume specific secret is needed whenever a volume is attached to a node for use by a pod.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When this happens, the Ondat node container&amp;rsquo;s &lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/&#34;&gt;ServiceAccount&lt;/a&gt; reads the secret and passes it to the Ondat Control Plane.&lt;/li&gt;
&lt;li&gt;A volume missing its key or with a malformed key will be unable to attach.&lt;/li&gt;
&lt;li&gt;The key is stored in memory by Ondat only on the node that the volume is being used on. As a result, encryption and decryption are performed &lt;strong&gt;where the data is consumed, rather than where it is stored&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Because of this, the use of encrypted volumes is transparent to the user. There is a complete integration between Kubernetes applications and Ondat volume encryption.&lt;/p&gt;
&lt;h3 id=&#34;encryption-key-management-best-practices&#34;&gt;Encryption Key Management Best Practices&lt;/h3&gt;
&lt;p&gt;Ondat saves volume encryption keys in Kubernetes secrets, thus - backups are imperative in case Kubernetes &lt;code&gt;etcd&lt;/code&gt; backing store is lost or damaged.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;⚠️ Ondat has no ability to decrypt a volume whose encryption keys have been lost.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Secrets in Kubernetes are not encrypted by default, they are stored in Kubernetes &lt;code&gt;etcd&lt;/code&gt; backing store in simple &lt;a href=&#34;https://en.wikipedia.org/wiki/Base64&#34;&gt;Base64&lt;/a&gt; encoding.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;As Ondat encryption keys are stored as Kubernetes secrets, this means that anyone with access to Kubernetes &lt;code&gt;etcd&lt;/code&gt; backing store can read encryption keys and decrypt volumes, unless the cluster is using an external secrets store for key management.&lt;/li&gt;
&lt;li&gt;For more information on how to enable and configure encryption of Kubernetes secrets data at rest, review the &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/&#34;&gt;official Kubernetes documentation here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Secrets are not garbage-collected by Ondat, therefore - to clean up completely upon deletion of a volume it is necessary to also delete that volume&amp;rsquo;s secret. There is no benefit to doing this, however.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;managing-keys-with-a-key-management-service-kms-provider&#34;&gt;Managing Keys With A Key Management Service (KMS) Provider&lt;/h3&gt;
&lt;p&gt;As mentioned in the section above, Ondat volume encryption keys are stored within Kubernetes &lt;code&gt;etcd&lt;/code&gt; backing store as Kubernetes secrets. Whilst Kubernetes &lt;code&gt;etcd&lt;/code&gt; and secrets can also be encrypted, many security-focused organisations choose to use an &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/kms-provider/&#34;&gt;external Key Management Service (KMS) provider for data encryption&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To address this from a Kubernetes limitations perspective and provide an agnostic solution, Ondat&amp;rsquo;s encryption design allows the user to leverage any supported &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/kms-provider/#implementing-a-kms-plugin&#34;&gt;Kubernetes KMS plugin&lt;/a&gt; to envelop the secrets into a KMS provider encryption scheme.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ondat enables end users to transparently integrate any supported KMS plugin with Ondat encryption key management using the standard Kubernetes API and Kubernetes KMS provider framework. The architecture diagram below provides a high level overview of the process.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/images/docs/gui-v2/kms-key-management.png&#34; alt=&#34;KMS Key Management&#34;&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The KMS plugin is deployed within the Kubernetes cluster.&lt;/li&gt;
&lt;li&gt;The KMS plugin is configured to act as a broker between the Kubernetes API server and the KMS server API endpoint.&lt;/li&gt;
&lt;li&gt;At volume creation, Ondat will create a Kubernetes secret using Kubernetes API calls&lt;/li&gt;
&lt;li&gt;The KMS plugin will handle the Kubernetes API Secret creation call and interface to the KMS server instance.&lt;/li&gt;
&lt;li&gt;The KMS server will return the secret using its encryption envelop scheme.&lt;/li&gt;
&lt;li&gt;The KMS plugin will store the encrypted secret within Kubernetes &lt;code&gt;etcd&lt;/code&gt; backing store.&lt;/li&gt;
&lt;/ol&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Ondat Feature Labels</title>
      <link>/docs/concepts/labels/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/concepts/labels/</guid>
      <description>
        
        
        &lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Ondat Feature labels are &lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/&#34;&gt;Kubernetes Labels&lt;/a&gt; which provide a powerful and flexible way to control storage features.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Applying specific feature labels triggers &lt;a href=&#34;/docs/concepts/compression/&#34;&gt;compression&lt;/a&gt;, &lt;a href=&#34;/docs/concepts/replication/&#34;&gt;replication&lt;/a&gt;, &lt;a href=&#34;/docs/concepts/encryption/&#34;&gt;data encryption&lt;/a&gt; and other storage features. In order to use feature labels, end users are required to explicitly enable the features they want to use in their cluster.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;types-of-ondat-feature-labels&#34;&gt;Types Of Ondat Feature Labels&lt;/h2&gt;
&lt;h3 id=&#34;ondat-volume-labels&#34;&gt;Ondat Volume Labels&lt;/h3&gt;
&lt;p&gt;Below are the list of available feature labels that can be used to define &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/volumes/&#34;&gt;Volume resources&lt;/a&gt; and &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource&#34;&gt;StorageClass resources&lt;/a&gt; in an Ondat cluster.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;💡 The &lt;strong&gt;encryption&lt;/strong&gt; and &lt;strong&gt;compression&lt;/strong&gt; labels can only applied at provisioning time, they can&amp;rsquo;t be changed during execution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Feature Name&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Label Reference&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Values&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Feature Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;/docs/concepts/compression/&#34;&gt;&lt;strong&gt;Compression&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;storageos.com/nocompress&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;true&lt;/code&gt; / &lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Enables or disables compression of data-at-rest and data-in-transit. Compression &lt;strong&gt;is not enabled by default&lt;/strong&gt; to maximise performance.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;/docs/concepts/encryption/&#34;&gt;&lt;strong&gt;Encryption&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;storageos.com/encryption&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;true&lt;/code&gt; / &lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Encrypts the contents of the volume. For each volume, a key is automatically generated, stored, and linked with the PVC.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;/docs/concepts/replication/#ondat-failure-modes&#34;&gt;&lt;strong&gt;Failure Mode&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;storageos.com/failure-mode&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;hard&lt;/code&gt;, &lt;code&gt;soft&lt;/code&gt;, &lt;code&gt;alwayson&lt;/code&gt;, or &lt;code&gt;threshold&lt;/code&gt; integers starting from &lt;code&gt;0&lt;/code&gt; to &lt;code&gt;5&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Sets the failure mode for a volume, either explicitly using a failure mode or implicitly using a replica threshold. The default setting of a failure mode is &lt;code&gt;hard&lt;/code&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;/docs/concepts/replication/&#34;&gt;&lt;strong&gt;Replication&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;storageos.com/replicas&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;integers&lt;/code&gt; starting from &lt;code&gt;0&lt;/code&gt; to &lt;code&gt;5&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Sets the number of replicas, for example full copies of the data across nodes. Typically &lt;code&gt;1&lt;/code&gt; or &lt;code&gt;2&lt;/code&gt; replicas is sufficient (&lt;code&gt;2&lt;/code&gt; or &lt;code&gt;3&lt;/code&gt; instances of the data). Latency implications need to be assessed when using &lt;strong&gt;more than&lt;/strong&gt; &lt;code&gt;2&lt;/code&gt; replicas.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;/docs/concepts/tap/&#34;&gt;&lt;strong&gt;Topology-Aware Placement&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;storageos.com/topology-aware&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;true&lt;/code&gt; / &lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Enables or disables Ondat Topology-Aware Placement.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;/docs/concepts/tap/#topology-domains&#34;&gt;&lt;strong&gt;Topology Domain Key&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;storageos.com/topology-key&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;custom region, read as a &lt;a href=&#34;https://en.wikipedia.org/wiki/String_%28computer_science%29&#34;&gt;string&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Define the failure domain for the node by using a custom key. If you don&amp;rsquo;t define a custom key, the label defaults to the &lt;code&gt;topology.kubernetes.io/zone&lt;/code&gt; value.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;ondat-node-labels&#34;&gt;Ondat Node Labels&lt;/h3&gt;
&lt;p&gt;When Ondat is run within Kubernetes, the &lt;a href=&#34;https://github.com/storageos/api-manager&#34;&gt;Ondat API Manager&lt;/a&gt; syncs any &lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/&#34;&gt;Kubernetes node labels&lt;/a&gt; to the corresponding Ondat node. The Kubernetes node labels act as the &amp;ldquo;source of truth&amp;rdquo;, so labels should be applied to the Kubernetes nodes rather than to Ondat nodes. This is because the Kubernetes node labels overwrite the Ondat node labels on sync.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Below are the list of available feature labels that can be used to define &lt;a href=&#34;https://kubernetes.io/docs/concepts/architecture/nodes/&#34;&gt;Kubernetes Nodes&lt;/a&gt; in an Ondat Cluster.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Feature Name&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Label Reference&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Values&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Feature Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;/docs/concepts/nodes/#compute-only-mode&#34;&gt;&lt;strong&gt;Compute-only Nodes&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;storageos.com/computeonly&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;true&lt;/code&gt; / &lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Specifies whether a node should be &lt;code&gt;computeonly&lt;/code&gt; where it only acts as a client and does not host volume data locally, otherwise the node is hyper-converged (the default), where the node can operate in both client and server modes.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;ondat-pod-labels&#34;&gt;Ondat Pod Labels&lt;/h3&gt;
&lt;p&gt;Below are the list of available feature labels that can be used to define &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/pods/&#34;&gt;Kubernetes Pods&lt;/a&gt; in an Ondat Cluster.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;💡 For a pod to be fenced by Ondat, a recommendation will be to review the the &lt;a href=&#34;/docs/operations/fencing&#34;&gt;Ondat Fencing&lt;/a&gt; operations page for more information.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Feature Name&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Label Reference&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Values&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Feature Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;/docs/concepts/fencing/&#34;&gt;&lt;strong&gt;Pod Fencing&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;storageos.com/fenced&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;true&lt;/code&gt; / &lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Targets a pod to be fenced in case of node failure. The default value is &lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;how-to-use-ondat-feature-labels&#34;&gt;How To Use Ondat Feature Labels?&lt;/h2&gt;
&lt;p&gt;For more information about how to enable specific Ondat features, review the Ondat Feature Labels operations pages listed below;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;/docs/operations/compute-only&#34;&gt;How To Setup A Centralised Cluster Topology&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/docs/operations/replication&#34;&gt;How To Use Volume Replication&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/docs/operations/failure-modes/&#34;&gt;How To Use Failure Modes&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/docs/operations/fencing/&#34;&gt;How To Enable Fencing&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/docs/operations/tap/&#34;&gt;How To Enable Topology-Aware Placement (TAP)&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/docs/operations/encryption/&#34;&gt;How To Enable Data Encryption&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/docs/operations/compression&#34;&gt;How To Enable Data Compression&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Ondat Fencing</title>
      <link>/docs/concepts/fencing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/concepts/fencing/</guid>
      <description>
        
        
        &lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;💡 This feature is available in release &lt;code&gt;v2.4.0&lt;/code&gt; or greater.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;what-is-ondat-fencing&#34;&gt;What Is Ondat Fencing?&lt;/h3&gt;
&lt;p&gt;In order to understand what Ondat Fencing for Kubernetes is and when it is needed, it is required to first understand the behaviour of &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/&#34;&gt;Kubernetes StatefulSets&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;StatefulSets&lt;/em&gt; are the de facto Kubernetes controller to use for stateful applications. The StatefulSet controller offers guarantees around pod uniqueness, sticky identities and the persistence of PVCs beyond the lifetime of their pods.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;As such, StatefulSets have different characteristics and provide different guarantees than &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/deployment/&#34;&gt;Kubernetes Deployments&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Deployments&lt;/em&gt; guarantee the amount of healthy replicas by reconciling towards the deployment desired state. Attempts to align the number of healthy pods with the deployment&amp;rsquo;s desired state happen as fast as possible by aggressively initialising and terminating pods.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If one pod is terminating, another will be automatically scheduled to start even if the first pod is not yet completely terminated. Stateless applications benefit from this behaviour as one pod executes the same work as any other in the deployment.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;StatefulSets, on the other hand, &lt;strong&gt;guarantee that every pod scheduled has a unique identity&lt;/strong&gt;, which is to say that only a single copy of a pod is running in the cluster at any one time.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Whenever scheduling decisions are made, the StatefulSet controller ensures that only one copy of this pod is running at any time.&lt;/li&gt;
&lt;li&gt;If a pod is deleted, a new pod will not be scheduled until the first pod is fully terminated. This is an important guarantee as file systems need to be unmounted before they can be remounted in a new pod. Any &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes&#34;&gt;ReadWriteOnce (RWO)&lt;/a&gt; PVC defining a device requires this behaviour to ensure the consistency of the data and thus the PVC.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To protect data integrity, Kubernetes guarantees that there will never be more than one instance of a StatefulSet pod running at a time. It assumes that when a node is determined to be offline it may still be running the workload but partitioned from the network. Since Kubernetes is unable to verify that the pod has been stopped it errors on the side of caution and does not allow a replacement to start on another node.&lt;/p&gt;
&lt;p&gt;Kubernetes does reschedule pods from some controllers when nodes become unavailable. The default behaviour is that when a node becomes unavailable its status becomes &lt;code&gt;Unknown&lt;/code&gt; and after the &lt;code&gt;pod-eviction-timeout&lt;/code&gt; has passed pods are scheduled for deletion. By default, the &lt;code&gt;pod-eviction-timeout&lt;/code&gt; is &lt;code&gt;300&lt;/code&gt; seconds.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For this reason, Kubernetes requires manual intervention to initiate timely failover of a StatefulSet pod. The &lt;strong&gt;Ondat Fencing Controller&lt;/strong&gt; gives the capability to enable fast failover for workloads when a node goes offline.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For more information on the rationale behind the design of StatefulSets, review the Kubernetes design proposal archive for &lt;a href=&#34;https://github.com/kubernetes/design-proposals-archive/blob/main/storage/pod-safety.md&#34;&gt;Pod Safety, Consistency Guarantees, and Storage Implications&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;ondat-fencing-controller&#34;&gt;Ondat Fencing Controller&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;💡 The Ondat Fencing Controller is part of the Ondat API Manager which is deployed in high availability mode when Ondat is installed.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;💡 High Availability for StatefulSet applications can be achieved with the Ondat Fencing feature.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Since Ondat is able to determine when a node is no longer able to access a volume and has protections in place to ensure that a partitioned or formerly partitioned node can stop writing data, it can work with Kubernetes to perform safe, fast failovers of pods, including those running in StatefulSets.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When Ondat detects that a node has gone offline or become partitioned, it marks the node offline and performs volume failover operations.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;a href=&#34;https://github.com/storageos/api-manager/tree/master/controllers/fencer&#34;&gt;Ondat Fencing Controller&lt;/a&gt; watches for these node failures and determines if there are any pods assigned to the failed node with the label &lt;code&gt;storageos.com/fenced=true&lt;/code&gt;, and if the pods have any PVCs backed by Ondat volumes.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When a pod has Ondat volumes and if they are all healthy, the Ondat Fencing Controller deletes the pod to allow it to be rescheduled on another node. It also deletes the &lt;code&gt;VolumeAttachment&lt;/code&gt; object for the corresponding volumes so that they can be immediately attached to the new node.&lt;/li&gt;
&lt;li&gt;No changes are made to pods that have Ondat volumes that are unhealthy. This is usually because a volume was configured to not have any replicas, and the node with the single copy of the data is offline. In this case it is better to wait for the node to recover.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ondat Fencing works with both dynamically provisioned PVCs and PVCs referencing pre-provisioned volumes.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In addition, the fencing feature is opt-in and pods must have the &lt;code&gt;storageos.com/fenced=true&lt;/code&gt; label set, and be using at least one Ondat volume, to enable fast failover.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For more information about how to enable Ondat fencing, review the &lt;a href=&#34;/docs/operations/fencing&#34;&gt;Ondat Fencing&lt;/a&gt; operations page.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Ondat Files</title>
      <link>/docs/concepts/rwx/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/concepts/rwx/</guid>
      <description>
        
        
        &lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;💡 This feature is available in release &lt;code&gt;v2.3.0&lt;/code&gt; or greater.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;what-is-ondat-files&#34;&gt;What Is Ondat Files?&lt;/h3&gt;
&lt;p&gt;Ondat provides support for &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes&#34;&gt;ReadWriteMany (RWX)&lt;/a&gt; persistent volumes.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A RWX PVC can be used simultaneously by many Pods in the same Kubernetes namespace for read and write operations.&lt;/li&gt;
&lt;li&gt;Ondat RWX persistent volumes are based on a &lt;a href=&#34;https://en.wikipedia.org/wiki/Clustered_file_system&#34;&gt;shared filesystem&lt;/a&gt;, the protocol being used for this feature&amp;rsquo;s backend is &lt;a href=&#34;https://en.wikipedia.org/wiki/Network_File_System&#34;&gt;Network Files System (NFS)&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ondat-files-architecture&#34;&gt;Ondat Files Architecture&lt;/h3&gt;
&lt;p&gt;For each RWX persistent volume, the following components below are required:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Ondat ReadWriteOnly (RWO) Volume&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Ondat provisions a standard &lt;a href=&#34;/docs/concepts/volumes&#34;&gt;volume&lt;/a&gt; that provides a block device for the file system of the NFS server.&lt;/li&gt;
&lt;li&gt;This means that every RWX Volume has its own RWO Volume - thus allowing RWX Volumes to leverage the synchronous &lt;a href=&#34;https://en.wikipedia.org/wiki/Replication_%28computing%29&#34;&gt;replication&lt;/a&gt; and automatic &lt;a href=&#34;https://en.wikipedia.org/wiki/Failover&#34;&gt;failover&lt;/a&gt; functionality of Ondat, providing the NFS server with high availability.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NFS-Ganesha Server&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;For each RWX volume, an &lt;a href=&#34;https://nfs-ganesha.github.io/&#34;&gt;NFS-Ganesha&lt;/a&gt; server is spawned by Ondat.&lt;/li&gt;
&lt;li&gt;The NFS server runs in user space on the bode containing the primary volume. Each NFS server uses its own &lt;em&gt;RWO&lt;/em&gt; volume to store data so the data of each Volume is isolated.&lt;/li&gt;
&lt;li&gt;Ondat binds an &lt;a href=&#34;https://en.wikipedia.org/wiki/Ephemeral_port&#34;&gt;ephemeral port&lt;/a&gt; to the host network interface for each NFS-Ganesha server.&lt;/li&gt;
&lt;li&gt;The NFS export is presented using &lt;a href=&#34;https://datatracker.ietf.org/doc/html/rfc7862&#34;&gt;&lt;code&gt;NFS v4.2&lt;/code&gt;&lt;/a&gt;. Ensure that you review the official &lt;a href=&#34;/docs/prerequisites/firewalls&#34;&gt;prerequisites&lt;/a&gt; page for more information on the port number range, that is for Ondat RWX persistent volumes to successfully run.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ondat API Manager&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;The Ondat API Manager resource monitors Ondat RWX volumes to create and maintain a &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/service/&#34;&gt;Kubernetes service&lt;/a&gt; that points towards each RWX volume&amp;rsquo;s NFS export endpoint.&lt;/li&gt;
&lt;li&gt;The API Manager is responsible for updating the service endpoint when a RWX volume failover occurs.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;how-are-ondat-readwritemany-rwx-persistentvolumeclaims-pvcs-provisioned&#34;&gt;How are Ondat ReadWriteMany (RWX) PersistentVolumeClaims (PVCs) Provisioned?&lt;/h3&gt;
&lt;p&gt;The sequence in which a RWX PVC is provisioned and used demonstrated in the steps below:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; (PVC) is created with &lt;code&gt;ReadWriteMany&lt;/code&gt; (RWX) access mode using any Ondat &lt;code&gt;StorageClass&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Ondat dynamically provisions the &lt;code&gt;PersistentVolume&lt;/code&gt; (PV).&lt;/li&gt;
&lt;li&gt;A new Ondat &lt;code&gt;ReadWriteOnly&lt;/code&gt; (RWO) Volume is provisioned internally (not visible in Kubernetes).&lt;/li&gt;
&lt;li&gt;When the RWX PVC is consumed by a pod, an NFS-Ganesha server is instantiated on the same node as the primary volume.&lt;/li&gt;
&lt;li&gt;The NFS-Ganesha server then uses the RWO Ondat volume as its backend disk.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;Ondat API Manager&lt;/em&gt; publishes the host IP and port for the NFS service endpoint, by creating a Kubernetes service that points to the NFS-Ganesha server export endpoint.&lt;/li&gt;
&lt;li&gt;Ondat issues a NFS mount on the Node where the Pod using the PVC is scheduled.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For more information on how to get started with Ondat Files, review the &lt;a href=&#34;/docs/operations/rwx&#34;&gt;ReadWriteMany (RWX)&lt;/a&gt; operations page.&lt;/p&gt;
&lt;h3 id=&#34;high-availability-for-ondat-files&#34;&gt;High Availability For Ondat Files&lt;/h3&gt;
&lt;p&gt;Ondat RWX volumes failover in the same way as standard Ondat RWO volumes.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The replica volume is promoted upon detection of node failure and the NFS-Ganesha server is started on the node containing the promoted replica.&lt;/li&gt;
&lt;li&gt;The Ondat API Manager updates the endpoint of the Volume&amp;rsquo;s NFS service, causing traffic to be routed to the URL of the new NFS-Ganesha server.&lt;/li&gt;
&lt;li&gt;The NFS client in the application node (where the user&amp;rsquo;s pod is running) automatically reconnects.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;further-information&#34;&gt;Further Information&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;All &lt;a href=&#34;/docs/concepts/labels/&#34;&gt;Ondat Feature Labels&lt;/a&gt; that work on RWO volumes will also work on RWX volumes.&lt;/li&gt;
&lt;li&gt;A Ondat RWX volume is matched one-to-one with a PVC. Therefore the Ondat RWX volume can only be accessed by pods in the &lt;strong&gt;same&lt;/strong&gt; Kubernetes namespace.&lt;/li&gt;
&lt;li&gt;Ondat RWX volumes support volume resize.
&lt;ul&gt;
&lt;li&gt;For more information on how to resize a volume, review the &lt;a href=&#34;/docs/operations/resize&#34;&gt;Volume Resize&lt;/a&gt; operations page.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Ondat Metric Exporter</title>
      <link>/docs/concepts/metric-exporter/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/concepts/metric-exporter/</guid>
      <description>
        
        
        &lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;💡 This feature is available in release &lt;code&gt;v2.8.0&lt;/code&gt; or greater.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;prometheus-metrics-for-ondat-volumes&#34;&gt;Prometheus Metrics for Ondat Volumes&lt;/h3&gt;
&lt;p&gt;Following the &lt;a href=&#34;https://prometheus.io/docs/instrumenting/exporters/&#34;&gt;exporter pattern&lt;/a&gt;, we maintain and distribute our own &lt;a href=&#34;https://prometheus.io/&#34;&gt;Prometheus&lt;/a&gt; exporter for monitoring and alerting of Ondat volumes. The metrics our exporter publishes include data on volume health, capacity and traffic.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Ondat metric exporter repository is open source and can be located on &lt;a href=&#34;https://github.com/ondat/metrics-exporter&#34;&gt;GitHub&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To get started with installing and configuring the exporter in your Ondat cluster, review the &lt;a href=&#34;/docs/operations/metric-exporter/&#34;&gt;metric exporter&amp;rsquo;s&lt;/a&gt; operations page for more information.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;⚠️ When setting up a &lt;a href=&#34;https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/getting-started.md&#34;&gt;ServiceMonitor&lt;/a&gt; resource, ensure that you create the rules in the same namespace as your Prometheus resource and have its &lt;code&gt;selector&lt;/code&gt; field match the labels of the services exposing metrics - review the &lt;a href=&#34;/docs/operations/metric-exporter/&#34;&gt;example ServiceMonitor resource&lt;/a&gt; manifest in the operations page for more information.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;alerting-rules-for-ondat-volumes&#34;&gt;Alerting Rules for Ondat Volumes&lt;/h3&gt;
&lt;p&gt;Ondat also distributes example alert rules for Ondat metrics using &lt;a href=&#34;https://prometheus.io/docs/alerting/latest/alertmanager/&#34;&gt;Alertmanager&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The alert rules manifest can be located in the &lt;a href=&#34;https://github.com/ondat/metrics-exporter/tree/main/alertmanager&#34;&gt;&lt;code&gt;alertmanager&lt;/code&gt; sub directory under the Ondat metric exporter&lt;/a&gt; repository.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;grafana-dashboard-for-ondat-volumes&#34;&gt;Grafana Dashboard for Ondat Volumes&lt;/h3&gt;
&lt;p&gt;In addition to the Ondat metric exporter project, we also distribute &lt;a href=&#34;https://grafana.com/grafana/dashboards/&#34;&gt;Grafana dashboards&lt;/a&gt; that allow end users to easily visualize and get insights into the status of Ondat volumes.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The dashboards can be also located in the &lt;a href=&#34;https://github.com/ondat/metrics-exporter/tree/main/grafana&#34;&gt;&lt;code&gt;grafana&lt;/code&gt; sub directory under the Ondat metric exporter&lt;/a&gt; repository.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;contributing&#34;&gt;Contributing&lt;/h2&gt;
&lt;p&gt;If end users have suggestions/ideas for metrics that they would like Ondat to gather by default or improve the Grafana dashboards and Alertmanager integration, contributions are welcome.&lt;/p&gt;
&lt;p&gt;You can reach out to us on the &lt;a href=&#34;https://slack.storageos.com/&#34;&gt;Ondat community slack workspace&lt;/a&gt; or review the &lt;a href=&#34;https://github.com/ondat/metrics-exporter/blob/main/CONTRIBUTING.md&#34;&gt;contributing guidelines&lt;/a&gt; in the Ondat metric exporter repository.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Ondat Nodes</title>
      <link>/docs/concepts/nodes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/concepts/nodes/</guid>
      <description>
        
        
        &lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;An Ondat node is any machine (virtual or physical) that is running the Ondat &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/&#34;&gt;daemonset&lt;/a&gt; pod. A node must be running a daemonset pod in order to consume and/or present storage.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Nodes can be run in several modes, describe below;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;hyper-converged-mode&#34;&gt;Hyper-converged Mode&lt;/h3&gt;
&lt;p&gt;By default Ondat nodes run in &lt;strong&gt;hyper-converged&lt;/strong&gt; mode. This means that the node hosts data from Ondat volumes and can present volumes to applications.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A hyper-converged node can store data from a volume and present volumes to applications regardless of whether the data for the volume consumed is placed on that node or is being served remotely.&lt;/li&gt;
&lt;li&gt;Remote volumes like this are handled by an internal protocol to present block device access to applications running on different nodes from the one to which their backing data store is attached.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ondat implements an extension of a Kubernetes Scheduler object that influences the placement of Pods on the same nodes as their data.&lt;/p&gt;
&lt;h3 id=&#34;compute-only-mode&#34;&gt;Compute-only Mode&lt;/h3&gt;
&lt;p&gt;Alternatively, a node can run in &lt;strong&gt;Compute-only&lt;/strong&gt; mode, which means no storage is consumed on the node itself and the node only presents volumes hosted by other nodes.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Volumes presented to applications running on compute only nodes are therefore all remote.&lt;/li&gt;
&lt;li&gt;Compute only nodes can be very useful for topologies where nodes are ephemeral and should not host data, but the ephemeral nodes host applications that require Ondat volumes.&lt;/li&gt;
&lt;li&gt;The nodes that are not intended to hold data, but just to present Ondat volumes, can be set as compute-only.&lt;/li&gt;
&lt;li&gt;A node can be marked as compute only at any point in time by adding the label &lt;code&gt;storageos.com/computeonly=true&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;More information on feature labels can be found under the &lt;a href=&#34;/docs/concepts/labels&#34;&gt;Ondat Feature Labels&lt;/a&gt; page.&lt;/p&gt;
&lt;h3 id=&#34;storage-mode&#34;&gt;Storage Mode&lt;/h3&gt;
&lt;p&gt;Finally, nodes can be set to storage mode. Nodes set to storage mode don&amp;rsquo;t present data locally - instead all data is accessed through the network.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This topology is enforced by tainting the relevant nodes to ensure that application workloads cannot be scheduled there.&lt;/li&gt;
&lt;li&gt;This mode is ideal for ensuring maximum stability of data access as the node is isolated from resource drains that may occur due to applications running alongside.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For redundancy purposes, in high load clusters it is ideal to have several nodes running in this mode.&lt;/p&gt;
&lt;h2 id=&#34;further-information&#34;&gt;Further Information&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Review the &lt;a href=&#34;/docs/concepts/cluster-topologies/&#34;&gt;Ondat Cluster Topologies&lt;/a&gt; feature page for more information on the supported cluster topologies that end users can leverage when designing storage-optimised clusters for their stateful applications.&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Ondat Replication</title>
      <link>/docs/concepts/replication/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/concepts/replication/</guid>
      <description>
        
        
        &lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;h3 id=&#34;how-does-ondats-replication-work&#34;&gt;How Does Ondat&amp;rsquo;s Replication Work?&lt;/h3&gt;
&lt;p&gt;Ondat replicates volumes across nodes for data protection and high availability. Synchronous &lt;a href=&#34;https://en.wikipedia.org/wiki/Replication_%28computing%29&#34;&gt;replication&lt;/a&gt; ensures strong consistency for applications such as databases and message queues, incurring one network round trip on writes.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The basic model for Ondat replication is of a master volume with distributed replicas. Each volume can be replicated between &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;5&lt;/code&gt; times, which are provisioned to &lt;code&gt;0&lt;/code&gt; to &lt;code&gt;5&lt;/code&gt; nodes, up to the number of remaining nodes in the cluster.&lt;/li&gt;
&lt;li&gt;In this diagram, the master volume &lt;code&gt;D&lt;/code&gt; was created on node &lt;code&gt;1&lt;/code&gt;, and two replicas, &lt;code&gt;D2&lt;/code&gt; and &lt;code&gt;D3&lt;/code&gt; on nodes &lt;code&gt;3&lt;/code&gt; and &lt;code&gt;5&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/images/docs/concepts/high-availability.png&#34; alt=&#34;Ondat Replication Diagram&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Step 1]&lt;/strong&gt; &amp;raquo; Data from the application is written to the master volume first (&lt;code&gt;D&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Step 2]&lt;/strong&gt; &amp;raquo; Data is then written in parallel to the replica volumes (&lt;code&gt;D2&lt;/code&gt; &amp;amp; &lt;code&gt;D3&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Step 3]&lt;/strong&gt; &amp;raquo; Master and replica volumes all acknowledge that data has been received and written&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Step 4]&lt;/strong&gt; &amp;raquo; A successful write operation is returned to the application.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For most applications, one replica is sufficient &lt;code&gt;storageos.com/replicas=1&lt;/code&gt;. All replication traffic on the wire is compressed using the &lt;a href=&#34;https://en.wikipedia.org/wiki/LZ4_%28compression_algorithm%29&#34;&gt;LZ4 (compression algorithm)&lt;/a&gt;, then streamed over &lt;code&gt;TCP/IP&lt;/code&gt; to target port &lt;code&gt;TCP/5703&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the master volume is lost, a replica is promoted to master (&lt;code&gt;D2&lt;/code&gt; or &lt;code&gt;D3&lt;/code&gt; above) and a new replica is created and synced on an available node (node &lt;code&gt;2&lt;/code&gt; or &lt;code&gt;4&lt;/code&gt;). This is transparent to the application and does not cause downtime.&lt;/li&gt;
&lt;li&gt;If a replica volume is lost and there are enough remaining nodes, a new replica is created and synced on an available node. While a new replica is created and being synced, the volume&amp;rsquo;s health will be marked as degraded.&lt;/li&gt;
&lt;li&gt;If the lost replica comes back online before the new replica has finished synchronising, then Ondat will calculate which of the two synchronising replicas has the smallest difference compared to the master volume and keep that replica.&lt;/li&gt;
&lt;li&gt;The same holds true if a master volume is lost and a replica is promoted to be the new master. If possible, a new replica will be created and begin to sync. Should the former master come back online it will be demoted to a replica and the replica will the smallest difference to the current master will be kept.&lt;/li&gt;
&lt;li&gt;While the replica count is controllable on a per-volume basis, some environments may prefer to set &lt;a href=&#34;/docs/concepts/labels&#34;&gt;default labels on the StorageClass&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ondats-delta-sync-algorithm&#34;&gt;Ondat&amp;rsquo;s Delta Sync Algorithm&lt;/h3&gt;
&lt;p&gt;Ondat implements a delta sync between a volume master and its replicas.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This means that if a replica for a volume goes offline, that when the replica comes back online only the regions with changed blocks need to be synchronised.&lt;/li&gt;
&lt;li&gt;This optimisation reduces the time it takes for replicas to catch up, improving volume resilience.&lt;/li&gt;
&lt;li&gt;Additionally, it reduces network and I/O bandwidth which can reduce costs when running in public clouds.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;how-to-use-ondats-volume-replication&#34;&gt;How to use Ondat&amp;rsquo;s Volume Replication?&lt;/h3&gt;
&lt;p&gt;For more information on how to use the volume replication feature, review the &lt;a href=&#34;/docs/operations/replication&#34;&gt;Volume Replication&lt;/a&gt; operations page.&lt;/p&gt;
&lt;h3 id=&#34;ondat-topology-aware-placement-tap&#34;&gt;Ondat Topology-Aware Placement (TAP)&lt;/h3&gt;
&lt;p&gt;Ondat Topology-Aware Placement (TAP) is a feature that enforces placement of data across failure domains to guarantee high availability.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TAP uses default labels on nodes to define failure domains. For instance, an &lt;a href=&#34;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html&#34;&gt;Availability Zone (AZ)&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For more information on the Topology-Aware Placement feature, review the &lt;a href=&#34;/docs/concepts/tap&#34;&gt;Ondat Topology-Aware Placement&lt;/a&gt; feature page.&lt;/p&gt;
&lt;h2 id=&#34;ondat-failure-modes&#34;&gt;Ondat Failure Modes&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;💡 This feature is available in release &lt;code&gt;v2.4.0&lt;/code&gt; or greater.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Ondat failure modes offer different guarantees with regards to a volume&amp;rsquo;s mode of operation in the face of replica failure. If the failure mode is not specified it defaults to &lt;code&gt;Hard&lt;/code&gt;. Volume failure modes can be dynamically updated at runtime.&lt;/p&gt;
&lt;h3 id=&#34;hard-failure-mode&#34;&gt;&lt;code&gt;hard&lt;/code&gt; Failure Mode&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;hard&lt;/code&gt; failure mode requires that the number of declared replicas matches the available number of replicas at all times.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If a replica fails Ondat will attempt creation of a new replica for 90 seconds. After 90s if the old replica is not available and a new replica cannot be provisioned, Ondat cannot guarantee that the data is stored on the number of multiple nodes requested by the user. Ondat will therefore set the volume to be read-only.&lt;/li&gt;
&lt;li&gt;If a volume has gone read-only there are two stages to making it read-write again. Firstly, sufficient replicas must be provisioned to match the desired replica count. Depending on your environment, additional nodes and/or disk capacity may be required for this. Secondly, the volume must be remounted - necessitating pod deletion/recreation in Kubernetes.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;storageos.com/failure-mode: hard
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Number Of Nodes Required For A &lt;code&gt;hard&lt;/code&gt; Failure Mode Setup&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When a node fails, a new replica is provisioned and synced as described above. To ensure that a new replica can always be created, an additional node should be available.&lt;/li&gt;
&lt;li&gt;To guarantee high availability using &lt;code&gt;storageos.com/failure-mode: hard&lt;/code&gt;, clusters using volumes with &lt;code&gt;1&lt;/code&gt; replica must have at least &lt;code&gt;3&lt;/code&gt; storage nodes.&lt;/li&gt;
&lt;li&gt;When using volumes with &lt;code&gt;2&lt;/code&gt; replicas, at least &lt;code&gt;4&lt;/code&gt; storage nodes, &lt;code&gt;3&lt;/code&gt; replicas, &lt;code&gt;5&lt;/code&gt; nodes, and so on.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Minimum number of storage nodes = 1 (primary) + N (replicas) + 1&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;soft-failure-mode&#34;&gt;&lt;code&gt;soft&lt;/code&gt; Failure Mode&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;soft&lt;/code&gt; failure mode allows a volume to continue serving I/O even when a replica goes offline and a new replica fails to provision.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;So long as there are &lt;code&gt;not less than max(1, N-1)&lt;/code&gt; available replicas where &lt;code&gt;N&lt;/code&gt; is the number of replicas for the volume.&lt;/li&gt;
&lt;li&gt;For example, if a volume with &lt;code&gt;2&lt;/code&gt; replicas loses &lt;code&gt;1&lt;/code&gt; replica, then I/O would continue to be served since &lt;code&gt;1&lt;/code&gt; replica remaining &lt;code&gt;&amp;gt;= max(1, 1)&lt;/code&gt;.
&lt;blockquote&gt;
&lt;p&gt;⚠️ If a volume with &lt;code&gt;1&lt;/code&gt; replica loses &lt;code&gt;1&lt;/code&gt; replica, then I/O would halt after &lt;code&gt;90&lt;/code&gt; seconds since &lt;code&gt;0&lt;/code&gt;
replicas remaining &lt;code&gt;&amp;lt; max(1, 0)&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;storageos.com/failure-mode: soft
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Number Of Nodes Required For A &lt;code&gt;soft&lt;/code&gt; Failure Mode Setup&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To ensure that a &lt;code&gt;storageos.com/failure-mode: soft&lt;/code&gt; volume is highly available, clusters using volumes with &lt;code&gt;1&lt;/code&gt; replica must have at least &lt;code&gt;2&lt;/code&gt; storage nodes.&lt;/li&gt;
&lt;li&gt;When using volumes with &lt;code&gt;2&lt;/code&gt; replicas, at least &lt;code&gt;3&lt;/code&gt; storage nodes, &lt;code&gt;3&lt;/code&gt; replicas, 3 nodes, etc.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Minimum number of storage nodes = 1 (primary) + N (replicas)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;threshold-failure-mode&#34;&gt;&lt;code&gt;threshold&lt;/code&gt; Failure Mode&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;threshold&lt;/code&gt; failure mode allows the user to set the minimum required number of online replicas for a volume.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For example for a volume with &lt;code&gt;2&lt;/code&gt; replicas, setting the threshold to &lt;code&gt;1&lt;/code&gt; would allow a single replica to be offline, whereas setting threshold to &lt;code&gt;0&lt;/code&gt; would allow &lt;code&gt;2&lt;/code&gt; replicas to be offline.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;storageos.com/failure-mode: &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;(&lt;/span&gt;0-5&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Number Of Nodes Required For A &lt;code&gt;threshold&lt;/code&gt; Failure Mode Setup&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The minimum number of nodes for a &lt;code&gt;threshold&lt;/code&gt; volume is determined by the threshold that is set.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Minimum number of storage nodes = 1 (primary) + T (threshold)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;alwayson-failure-mode&#34;&gt;&lt;code&gt;alwayson&lt;/code&gt; Failure Mode&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;alwayson&lt;/code&gt; failure mode allows all replicas for a volume to be offline and keeps the volume writeable. A volume with failure mode AlwaysOn will continue to serve I/O regardless of how many replicas it currently has.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This mode should be used with caution as it effectively allows for only a single copy of the data to be available.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;storageos.com/failure-mode: alwayson
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Number Of Nodes Required For A &lt;code&gt;alwayson&lt;/code&gt; Failure Mode Setup&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;code&gt;storageos.com/failure-mode: alwayson&lt;/code&gt; volume is highly available albeit at the cost of reliability.&lt;/li&gt;
&lt;li&gt;The minimum node count here is &lt;code&gt;1&lt;/code&gt; as the loss of all replicas will be tolerated.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Minimum number of storage nodes = 1 (primary)&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For more information on how to use failure mode labels on volumes, review the &lt;a href=&#34;/docs/operations/failure-modes&#34;&gt;Failure Modes&lt;/a&gt; operations page.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Ondat Rolling Upgrades Protection For Orchestrators</title>
      <link>/docs/concepts/rolling-upgrades/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/concepts/rolling-upgrades/</guid>
      <description>
        
        
        &lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;💡 This feature is currently available as a Technical Preview from release &lt;code&gt;2.7.0&lt;/code&gt; or greater.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;rolling-upgrades-protection&#34;&gt;Rolling Upgrades Protection&lt;/h3&gt;
&lt;p&gt;You can use our rolling upgrade protection feature to upgrade your cluster&amp;rsquo;s orchestrator without causing downtime or failure of Ondat.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the volumes containing the data for your stateful workloads do not wait to successfully synchronize in-between nodes upgrading, this can potentially cause data inconsistency and downtime. As such it is necessary to perform these upgrades intelligently.&lt;/li&gt;
&lt;li&gt;We are developing a solution to this problem for you. It is currently a Technical Preview but now, for example, Ondat can support a &lt;a href=&#34;/docs/install/anthos/&#34;&gt;Google Anthos&lt;/a&gt; one-click upgrade without any downtime.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To get started with Ondat&amp;rsquo;s Rolling Upgrades Protection for your cluster, review the &lt;a href=&#34;/docs/operations/using-rolling-upgrades&#34;&gt;Platform Upgrade&lt;/a&gt; page for more information.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Ondat Snapshots</title>
      <link>/docs/concepts/snapshots/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/concepts/snapshots/</guid>
      <description>
        
        
        &lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;💡 This feature is available in release &lt;code&gt;v2.8.0&lt;/code&gt; or greater.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The Ondat Snapshot feature can be used in conjunction with &lt;a href=&#34;https://www.kasten.io/product/&#34;&gt;Kasten K10&lt;/a&gt; to snapshot, backup and restore Kubernetes applications.&lt;/p&gt;
&lt;p&gt;The snapshot functionality is useful for:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Disaster_recovery&#34;&gt;Disaster Recovery (DR)&lt;/a&gt; scenarios.&lt;/li&gt;
&lt;li&gt;Rolling back unwanted changes.&lt;/li&gt;
&lt;li&gt;Auditing purposes.&lt;/li&gt;
&lt;li&gt;Migrating applications between clusters.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;what-are-snapshots-backups--restores&#34;&gt;What Are Snapshots, Backups &amp;amp; Restores?&lt;/h3&gt;
&lt;p&gt;A “&lt;strong&gt;snapshot&lt;/strong&gt;” is a point-in-time copy of a PVC. Snapshots are modelled via the &lt;code&gt;VolumeSnapshot&lt;/code&gt; and &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; Kubernetes API objects.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Snapshots have limited use as they live within the cluster and cannot be used to restore the PVC if the node holding the snapshot is lost.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A “&lt;strong&gt;backup&lt;/strong&gt;” is the process of materialising a new PVC, whose data source is a previously created snapshot and then extracting the data to a location outside of the cluster.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Ondat Snapshots feature integrates with Kasten K10 to provide backup functionality.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A “&lt;strong&gt;restore&lt;/strong&gt;” is the process of restoring an application from a given backup.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Ondat Snapshots feature integrates with Kasten K10 to provide restore functionality.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;how-does-it-work&#34;&gt;How Does It Work?&lt;/h3&gt;
&lt;p&gt;The Kubernetes &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/volume-snapshots/&#34;&gt;Volume Snapshots&lt;/a&gt; feature provides users with a set of custom resource definitions (CRD) and APIs to create and manage volume snapshots. Storage providers can then implement the necessary &lt;a href=&#34;https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/&#34;&gt;Container Storage Interface (CSI)&lt;/a&gt; APIs to integrate with this feature.&lt;/p&gt;
&lt;p&gt;This is exactly what we’ve done at Ondat. Additional backup tooling, like Kasten K10, can then be utilised to orchestrate and automate snapshotting, backups and restores.&lt;/p&gt;
&lt;p&gt;To get started with installing and configuring the Ondat Snapshot feature in your Ondat cluster with Kasten K10, review the &lt;a href=&#34;/docs/operations/backups-and-restores-with-kastenk10/&#34;&gt;Snapshots&lt;/a&gt; operations page for more information.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;⚠️ The Ondat Snapshots feature is not fully CSI compliant yet. As of today, the feature can only be used with Kasten K10 and with restoration from an external backup.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;current-scope--limitations&#34;&gt;Current Scope &amp;amp; Limitations&lt;/h3&gt;
&lt;p&gt;The Ondat Snapshots feature has the following limitations:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The feature has been designed to work with &lt;strong&gt;Kasten K10&lt;/strong&gt; only. This is not a fully CSI compliant implementation of the specification yet.&lt;/li&gt;
&lt;li&gt;Restoring via Kasten 10 from a “local snapshot” is not supported with the Ondat Snapshot feature. Users may only restore applications using a Kasten K10
“External backup”.&lt;/li&gt;
&lt;li&gt;Snapshotting &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes&#34;&gt;ReadWriteMany (RWX)&lt;/a&gt; volumes is not supported. This is because it is next to impossible to ensure that a NFS mounted volume is in a suitable state for snapshotting.
&lt;ol&gt;
&lt;li&gt;For RWX volumes, the user only has access to the filesystem on the NFS client. It is not possible to run &lt;a href=&#34;https://man7.org/linux/man-pages/man8/fsfreeze.8.html&#34;&gt;&lt;code&gt;fsfreeze&lt;/code&gt;&lt;/a&gt; on this mount point &amp;ndash; NFS does not support it. Thus the user can not &lt;a href=&#34;https://en.wikipedia.org/wiki/Quiesce&#34;&gt;quiesce&lt;/a&gt; the filesystem and we can not take a &amp;ldquo;consistent&amp;rdquo; snapshot.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Ondat Topology-Aware Placement (TAP)</title>
      <link>/docs/concepts/tap/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/concepts/tap/</guid>
      <description>
        
        
        &lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;💡 This feature is available in release &lt;code&gt;v2.5.0&lt;/code&gt; or greater.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Ondat Topology-Aware Placement (TAP) is a feature that enforces placement of data across failure domains to guarantee high availability.&lt;/p&gt;
&lt;p&gt;Ondat TAP uses default &lt;a href=&#34;https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#built-in-node-labels&#34;&gt;labels on nodes&lt;/a&gt; to define failure domains - for instance, an &lt;a href=&#34;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html&#34;&gt;Availability Zone&lt;/a&gt;. However, the key label used to segment failure domains can be defined by the user per node. Lastly, Ondat TAP is an opt-in feature per volume.&lt;/p&gt;
&lt;h3 id=&#34;how-does-ondat-topology-aware-placement-work&#34;&gt;How does Ondat Topology-Aware Placement Work?&lt;/h3&gt;
&lt;p&gt;Ondat&amp;rsquo;s Topology-Aware Placement attempts to distribute sensitive data across different failure domains. Hence, a primary volume and its replicas are scattered across failure domains - that is implemented following a &lt;a href=&#34;https://en.wikipedia.org/wiki/Best-effort_delivery&#34;&gt;best effort algorithm&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In case that Ondat TAP rules can&amp;rsquo;t be fulfilled the placement algorithm will attempt a best approach placement (even if new replicas are in the same failure domain).&lt;/li&gt;
&lt;li&gt;The best effort placement allows the system to place replicas on the same failure domains when a full domain has failed catastrophically. Hence, the system self heals as fast as possible without waiting for the nodes on the failed domain to recover.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is the user&amp;rsquo;s responsibility to rebalance the data when the failed domain has recovered its availability. That can be achieved by recreating the replicas of a volume.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;💡 Future versions of Ondat will facilitate the procedure by allowing a volume drain.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;advantages-of-using-ondat-topology-aware-placement-tap&#34;&gt;Advantages of using Ondat Topology-Aware Placement (TAP)&lt;/h3&gt;
&lt;p&gt;Deploying a stateful application on a clusters with multiple nodes without Ondat TAP enabled can result in suboptimal placement for &lt;a href=&#34;https://en.wikipedia.org/wiki/High_availability&#34;&gt;high availability.&lt;/a&gt; Not enabling Ondat TAP can cause following problems:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unschedulable pods due to resource, affinity, and taint issues when a full failure domain experiences a failure.&lt;/li&gt;
&lt;li&gt;Volume replicas placed within the same zone as a primary volume.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/images/docs/concepts/tap.png&#34; alt=&#34;Ondat Topology-Aware Placement&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;how-to-use-ondat-topology-aware-placement&#34;&gt;How to use Ondat Topology-Aware Placement?&lt;/h3&gt;
&lt;p&gt;Topology-Aware Placement can be enabled by applying the label &lt;code&gt;storageos.com/topology-aware=true&lt;/code&gt; to a PVC or as a parameter of its StorageClass.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For more information on how to enable Ondat Topology-Aware Placement for your volumes, review the &lt;a href=&#34;/docs/operations/tap&#34;&gt;Ondat Topology-Aware Placement&lt;/a&gt; operations page.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;understanding-topology-domains&#34;&gt;Understanding Topology Domains&lt;/h3&gt;
&lt;p&gt;A topology domain is a set of nodes. The domain is identified by a label, which can be defined by the user.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The default label that Ondat uses to segment nodes in failure domains is &amp;raquo; &lt;code&gt;topology.kubernetes.io/zone&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;However, you can define your own topology key by setting the key string in the &lt;a href=&#34;/docs/concepts/labels/&#34;&gt;Ondat feature label&lt;/a&gt; &amp;raquo; &lt;code&gt;storageos.com/topology-key&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ondat-failure-modes--topology-aware-placement&#34;&gt;Ondat Failure Modes &amp;amp; Topology-Aware Placement&lt;/h3&gt;
&lt;p&gt;Failure modes are a complimentary feature of the Topology-Aware Placement functionality. Failure modes allow you to define how many replicas of a volume can become unavailable before the volume is marked as read-only.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For more information on how to Failure Modes work , review the &lt;a href=&#34;/docs/concepts/replication&#34;&gt;Ondat Topology-Aware Placement&lt;/a&gt; feature page.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, assuming that your cluster has three topology zones, &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;B&lt;/code&gt; and &lt;code&gt;C&lt;/code&gt;, and your deployment has a master and two replicas, Ondat will attempt to place one volume in each topology zone.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If zone &lt;code&gt;A&lt;/code&gt; fails, I/O operations to your volume will stop completely - if the Failure Mode is &lt;code&gt;hard&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;If the Failure Mode is &lt;code&gt;soft&lt;/code&gt; - I/O operations will continue while volume failover is in progress, and a new replica will be placed in an operational zone.&lt;/li&gt;
&lt;li&gt;Note that if zone &lt;code&gt;A&lt;/code&gt; recovers, the cluster will &lt;strong&gt;not&lt;/strong&gt; automatically rebalance.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;code&gt;soft&lt;/code&gt; failure mode will not tolerate the failure of multiple replicas at once, and will suspend I/O operations in this case.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If you wish to tolerate more than one failed replica, then you can set this as an integer using the &lt;code&gt;&amp;lt;integer&amp;gt;&lt;/code&gt; label.&lt;/li&gt;
&lt;li&gt;If individual nodes within a topology zone fail, the replicas will fail over to other nodes within that zone. Once nodes in the zone are exhausted, placement will revert to best-effort.&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Ondat Volumes</title>
      <link>/docs/concepts/volumes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/concepts/volumes/</guid>
      <description>
        
        
        &lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Ondat volumes are a logical construct which represent a writeable volume and exhibit standard POSIX semantics. Ondat presents volumes as mounts into containers via the &lt;a href=&#34;https://en.wikipedia.org/wiki/LIO_(SCSI_target)&#34;&gt;Linux-IO (LIO)&lt;/a&gt; subsystem.&lt;/p&gt;
&lt;p&gt;Conceptually, Ondat volumes have a frontend presentation, which is what the application sees, and a backend presentation, which is the actual on-disk format. Depending on the configuration, frontend and backend components may be on the same or different hosts.&lt;/p&gt;
&lt;p&gt;Volumes are formatted using the linux standard &lt;code&gt;ext4&lt;/code&gt; filesystem by default. Kubernetes users may change the default filesystem type to &lt;code&gt;ext2&lt;/code&gt;, &lt;code&gt;ext3&lt;/code&gt;, &lt;code&gt;ext4&lt;/code&gt;, or &lt;code&gt;xfs&lt;/code&gt; by setting the &lt;code&gt;fsType&lt;/code&gt; parameter in their &lt;code&gt;StorageClass&lt;/code&gt; - review the &lt;a href=&#34;/docs/reference/filesystems#persistent-volume-filesystems&#34;&gt;Supported Filesystems&lt;/a&gt; page for more information.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;💡 Different filesystems may be supported in the future.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Ondat volumes are represented on disk in two parts.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Actual volume data is written to blob files in &lt;code&gt;/var/lib/storageos/data/dev[\d+]&lt;/code&gt;. Inside these directories, each Ondat block device gets two blob files of the form &lt;code&gt;vol.xxxxxx.y.blob&lt;/code&gt;, where &lt;code&gt;x&lt;/code&gt; is the inode number for the device, and &lt;code&gt;y&lt;/code&gt; is an index between &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;We provide two blob files in order to ensure that certain operations which require locking do not impede in-flight writes to the volume.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In systems which have multiple &lt;code&gt;/var/lib/storageos/data/dev[\d+]&lt;/code&gt; directories, two blob files are created per block device.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This allows us to load-balance writes across multiple devices. In cases where dev directories are added after a period of runtime, later directories are favoured for writes until the data is distributed evenly across the blob files.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Metadata is kept in directories named &lt;code&gt;/var/lib/storageos/data/db[\d+]&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We maintain an index of all blocks written to the blob file inside the metadata store, including checksums. These checksums allow us to detect &lt;a href=&#34;https://en.wikipedia.org/wiki/Data_degradation&#34;&gt;bit rot&lt;/a&gt;, and return errors on reads, rather than serve bad data.&lt;/li&gt;
&lt;li&gt;In future versions we may implement recovery from replicas for volumes with one or more replicas defined.&lt;/li&gt;
&lt;li&gt;Ondat metadata requires approximately &lt;code&gt;2.7 GiB&lt;/code&gt; of storage per &lt;code&gt;1 TiB&lt;/code&gt; of allocated blocks in the associated volume. This size is consistent irrespective of data compression defined on the volume.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To ensure deterministic performance, individual Ondat volumes must fit on a single node.&lt;/p&gt;
&lt;h3 id=&#34;minimum-ondat-volume-size&#34;&gt;Minimum Ondat Volume Size&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The minimum volume size Ondat supports is &lt;code&gt;1 GiB&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ondat-volume-resizing&#34;&gt;Ondat Volume Resizing&lt;/h3&gt;
&lt;p&gt;Ondat supports offline resize of volumes.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This means that a volume cannot be resized while it is in use. Furthermore, in order for a resize operation to take place the volume must not be attached to a node. This is to ensure that the volume is not in use.&lt;/li&gt;
&lt;li&gt;This means that if a Kubernetes pod is currently consuming a volume that a resize request has been issued for, the resize will not be actioned until the pod is terminated and the volume is detached from the node.&lt;/li&gt;
&lt;li&gt;The Ondat Control Plane will then attach the volume to the node that holds the master deployment and resize the underlying block device and then run &lt;a href=&#34;https://man7.org/linux/man-pages/man8/resize2fs.8.html&#34;&gt;resize2fs&lt;/a&gt; to expand the filesystem.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For more information on how to resize a volume, review the &lt;a href=&#34;/docs/operations/resize&#34;&gt;Volume Resize&lt;/a&gt; operations page.&lt;/p&gt;
&lt;h3 id=&#34;ondat-volume-encryption&#34;&gt;Ondat Volume Encryption&lt;/h3&gt;
&lt;p&gt;Volumes can be configured on creation to have &lt;a href=&#34;https://en.wikipedia.org/wiki/Data_at_rest&#34;&gt;data encryption-at-rest&lt;/a&gt;. Data is encrypted with XTS-AES and decrypted upon use.&lt;/p&gt;
&lt;p&gt;For more information on how to enable data encryption for Ondat volumes, review the &lt;a href=&#34;/docs/concepts/encryption&#34;&gt;Ondat Data Encryption&lt;/a&gt; feature page.&lt;/p&gt;
&lt;h3 id=&#34;trim&#34;&gt;TRIM&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;💡 This feature is available in release &lt;code&gt;v2.4.0&lt;/code&gt; or greater.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Ondat volumes support &lt;a href=&#34;https://en.wikipedia.org/wiki/Trim_%28computing%29&#34;&gt;TRIM/UNMAP&lt;/a&gt; which allows the space allocated to deleted blocks to be reclaimed from the backend blob files that back each volume when a TRIM call is made.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Support for TRIM is enabled by default for all uncompressed volumes, volumes are created without compression enabled by default.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For more information on how to TRIM a filesystem, review the &lt;a href=&#34;/docs/operations/trim&#34;&gt;TRIM&lt;/a&gt; operations page.&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
