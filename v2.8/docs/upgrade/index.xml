<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> ‚Äì Upgrade</title>
    <link>https://docs.ondat.io/v2.8/docs/upgrade/</link>
    <description>Recent content in Upgrade on </description>
    <generator>Hugo -- gohugo.io</generator>
    
	  <atom:link href="https://docs.ondat.io/v2.8/docs/upgrade/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Docs: Ondat Upgrade</title>
      <link>https://docs.ondat.io/v2.8/docs/upgrade/upgrade/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.ondat.io/v2.8/docs/upgrade/upgrade/</guid>
      <description>
        
        
        &lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;This guide provides instructions on how to upgrade Ondat.&lt;/p&gt;
&lt;h2 id=&#34;upgrading-an-ondat-v2-cluster&#34;&gt;Upgrading An Ondat &lt;code&gt;v2&lt;/code&gt; Cluster&lt;/h2&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;‚ö†Ô∏è Ensure that you have read the &lt;a href=&#34;https://docs.ondat.io/v2.8/docs/prerequisites/pidlimits&#34;&gt;PIDs prerequisite introduced in Ondat
v2.3&lt;/a&gt; and that you have checked the init
container logs to ensure your environments PID limits are set correctly.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;üí° Pull down the new Ondat container image &lt;code&gt;storageos/node:v2.8.2&lt;/code&gt; onto the
nodes beforehand so that the cluster spins up faster.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;üí° Speak with our support team &lt;a href=&#34;https://docs.ondat.io/v2.8/docs/support/&#34;&gt;here&lt;/a&gt; so we can assist you
with your upgrade.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;procedure&#34;&gt;Procedure&lt;/h2&gt;
&lt;h3 id=&#34;step-1---backup-ondat-deployment-manifests&#34;&gt;Step 1 - Backup Ondat Deployment Manifests&lt;/h3&gt;
&lt;p&gt;Make sure that you keep a backup of all the Ondat YAML files. You can also
backup the &lt;code&gt;StatefulSet&lt;/code&gt; yaml files to keep track of the replicas.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl get pod -n storageos -o yaml &amp;gt; storageos_operator.yaml
kubectl get storageoscluster -n storageos -o yaml &amp;gt; storageos_cr.yaml
kubectl get statefulset --all-namespaces &amp;gt; statefulset-sizes.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;step-2---scale-down-stateful-applications-to-zero&#34;&gt;Step 2 - Scale Down Stateful Applications To Zero&lt;/h3&gt;
&lt;p&gt;Scale all of the stateful applications that use Ondat volumes to 0.&lt;/p&gt;
&lt;h3 id=&#34;step-3---upgrade-ondat&#34;&gt;Step 3 - Upgrade Ondat&lt;/h3&gt;
&lt;p&gt;Run the following command using the kubectl storageos plugin.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Make sure you are using the latest version of the kubectl storageos plugin.
You can make use of the &lt;a href=&#34;https://docs.ondat.io/v2.8/docs/reference/kubectl-plugin/&#34;&gt;installation guide&lt;/a&gt;
and get the latest version
&lt;a href=&#34;https://github.com/storageos/kubectl-storageos&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl storageos upgrade
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;üí° Please use the &lt;code&gt;--etcd-tls-enabled&lt;/code&gt; if using TLS with your ETCD.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;üí° If you are using a namespace other than &lt;code&gt;storageos&lt;/code&gt; for your Ondat
install, please use &lt;code&gt;--uninstall-stos-operator-namespace&lt;/code&gt; argument because it
uninstalls the cluster first and then reinstalls it with the new version.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;üí° If at any point something goes wrong with the upgrade process, backups of
all the relevant Kubernetes manifests can be found in &lt;code&gt;~/.kube/storageos/&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;step-4---scale-up-stateful-applications&#34;&gt;Step 4 - Scale Up Stateful Applications&lt;/h3&gt;
&lt;p&gt;Once the Ondat upgrade is complete and the core components are back online,
scale up the stateful applications that use Ondat volumes back up to their
respective replica count.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Platform Upgrade</title>
      <link>https://docs.ondat.io/v2.8/docs/upgrade/using-rolling-upgrades/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.ondat.io/v2.8/docs/upgrade/using-rolling-upgrades/</guid>
      <description>
        
        
        &lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;This guide will demonstrate how to enable protection for your orchestrator&amp;rsquo;s rolling upgrades using the &lt;a href=&#34;https://docs.ondat.io/v2.8/docs/concepts/rolling-upgrades/#node-guard&#34;&gt;Node Guard&lt;/a&gt; and &lt;a href=&#34;https://docs.ondat.io/v2.8/docs/concepts/rolling-upgrades/#node-manager&#34;&gt;Node Manager&lt;/a&gt;. This feature helps prevent your persistent storage volumes from becoming unhealthy during the rolling downtime of an orchestrator upgrade.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;‚ö†Ô∏è This feature is currently in tech preview, we only recommend using this feature on your test clusters.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;‚ö†Ô∏è Make sure you have met the requirements of &lt;a href=&#34;https://kubernetes.io/docs/tasks/run-application/configure-pdb/&#34;&gt;configuring a Pod Disruption Budget (PDB)&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;‚ö†Ô∏è If your volume does not have any replicas, the rolling upgrades feature will not start on any StorageOS node until you have one or more replicas on all your volumes.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;‚ö†Ô∏è This feature supports the following platforms: Google Anthos, and Google GKE with future support to be expanded to Amazon EKS, Openshift, and Rancher.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;‚ö†Ô∏è Using Ondat for the internal registry is not recommended. OpenShift requires the internal registry to be available but Ondat volumes may become unavailable during the upgrade.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;‚ö†Ô∏è For Openshift: The PDB feature is only stable in Kubernetes v1.21+ and Openshift v4.8+.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;procedure&#34;&gt;Procedure&lt;/h2&gt;
&lt;h3 id=&#34;step-1---enable-node-manager--node-guard&#34;&gt;Step 1 - Enable Node Manager &amp;amp; Node Guard&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Add the following lines to the &lt;code&gt;StorageOSCluster&lt;/code&gt; spec:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;nodeManagerFeatures&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;   &lt;/span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;nodeGuard&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;Alternatively, you can run the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl get storageoscluster -n storageos storageoscluster -o yaml &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;|&lt;/span&gt; sed -e &lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#39;s|^spec:$|spec:\n  nodeManagerFeatures:\n    nodeGuard: &amp;#34;&amp;#34;|&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;|&lt;/span&gt; kubectl apply -f - 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;You will see new pods getting created, one pod per node in a cluster called Node Manager. If you enable the Node Guard during the first installation, Node Guard might fall into a temporary &lt;code&gt;CrashLoopBackoff&lt;/code&gt; loop until all cluster components are up and running.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Node Guard has a few configuration options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;MINIMUM_REPLICAS&lt;/code&gt;: minimum replica number of any volume. Default: 1&lt;/li&gt;
&lt;li&gt;&lt;code&gt;WATCH_ALL_VOLUMES&lt;/code&gt;: watch all volumes on every node, otherwise Node Guard watches volumes and their replicas on the node where it is running. Extra safety option with a performance impact. Default: false&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;nodeManagerFeatures&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;   &lt;/span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;nodeGuard&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#34;MINIMUM_REPLICAS=2,WATCH_ALL_VOLUMES=true&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;step-2---rolling-upgrades-is-ready&#34;&gt;Step 2 - Rolling Upgrades Is Ready&lt;/h3&gt;
&lt;p&gt;Congratulations, you are now ready to start the rolling upgrade process of your orchestrator!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;‚ö†Ô∏è GKE and AKS take care of the pod disruption budget for one hour. After this period, they drain the node, which would destroy the volume.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;‚ö†Ô∏è EKS takes care of PDB for 50 mins, after this period upgrade would fail unless it was forced.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;‚ö†Ô∏è Node Guard has a one-day termination period by default. The final termination period heavily depends on the platform you use. During the termination period, you should SSH into the node to create a backup in the worst-case scenario.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;troubleshooting&#34;&gt;Troubleshooting&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Volumes are healthy, all in sync but &lt;code&gt;storageos-node-manager&lt;/code&gt; pod is hanging on the &lt;code&gt;Terminating&lt;/code&gt; state.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The long termination period of Node Manager tries to keep failed node - and volumes on it - up and running as long as possible. This gives a chance to create a backup from an accidentally deleted machine. In case, Node Guard isn&amp;rsquo;t able to determine volume statuses, because of a network issue or missing StorageOS service, you have to delete pod manually by executing the following command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl delete pods -n storageos storageos-node-manager-XYZ --grace-period&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;0&lt;/span&gt; --force
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;A node has been removed accidentally or not in the official &lt;code&gt;graceful termination&lt;/code&gt; way before drain, and two Node Manager pods - one in &lt;code&gt;Pending&lt;/code&gt; and the other in &lt;code&gt;Terminating&lt;/code&gt; states - are hanging on the same node.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Node Manager deployment tolerates almost every issue on the target node to protect your data. On the other hand, Node Manager doesn&amp;rsquo;t tolerate itself on the same node. If a node goes down before Kubernetes was able to properly delete StorageOS Node daemonset, after the termination phase it re-schedules Node Manager pod to ensure the right number of replicas. But the pod isn&amp;rsquo;t able to be scheduled, because of the toleration. Meantime Kubernetes isn&amp;rsquo;t able to remove the pod in &lt;code&gt;Terminating&lt;/code&gt; state, because Kubelet isn&amp;rsquo;t responding.&lt;/p&gt;
&lt;p&gt;The only way to solve this situation is to delete the node from Kubernetes cluster by executing the command below:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl delete node XYZ
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;‚ö†Ô∏è Kubernetes has introduced Non-Graceful Node Shutdown Alpha in 1.24. This new feature allows cluster admins to mark failing nodes as &lt;code&gt;NoExecute&lt;/code&gt; or &lt;code&gt;NoScedule&lt;/code&gt;. Both options should solve the scheduling issue of Node Manager pod by decreasing the number of daemonset instances to the right number at Kubernetes API level, but in the absence of Kubelet termination of pods would still hangin.&lt;/p&gt;
&lt;/blockquote&gt;

      </description>
    </item>
    
  </channel>
</rss>
